{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73c394c",
   "metadata": {},
   "source": [
    "## teste 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a6b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import pandas as pd\n",
    "from utils.limpeza import carregar_dados\n",
    "from utils.seletores import selecionar_colunas_recomendador\n",
    "from app.recomendador import RecomendadorRH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5db8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dados carregados com sucesso: 5217 linhas e 403 colunas.\n"
     ]
    }
   ],
   "source": [
    "# Carrega o dataset completo e seleciona as colunas relevantes\n",
    "df = carregar_dados()\n",
    "df_selecionado = selecionar_colunas_recomendador(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4e9d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Filtros dispon√≠veis:\n",
      "- cargo_geral\n",
      "- cargo_atual\n",
      "- cargo_desejado\n",
      "- senioridade\n",
      "- experiencia_dados\n",
      "- experiencia_ti\n",
      "- modelo_trabalho_atual\n",
      "- modelo_trabalho_ideal\n",
      "- faixa_salarial\n",
      "- uf_onde_mora\n",
      "- regiao_onde_mora\n",
      "- satisfacao\n",
      "- linguagens_programacao\n",
      "- bancos_de_dados\n",
      "- clouds\n",
      "- bi_tools\n",
      "- planos_mudar_emprego\n",
      "- criterios_escolha_emprego\n"
     ]
    }
   ],
   "source": [
    "from utils.filtros import FILTROS_DISPONIVEIS\n",
    "\n",
    "# Exibir todas as chaves (nomes dos filtros)\n",
    "print(\"üîé Filtros dispon√≠veis:\")\n",
    "for chave in FILTROS_DISPONIVEIS:\n",
    "    print(\"-\", chave)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e36921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOG] Aplicando filtros com interse√ß√£o (AND):\n",
      "‚úÖ Cargo geral 'cientista': 913 matches\n",
      "üîç Processando filtro simples: modelo_trabalho_ideal\n",
      "‚úÖ 2238 matches em 2.s_modelo_de_trabalho_ideal\n",
      "üîß Processando m√∫ltiplos valores para linguagens_programacao\n",
      "‚úÖ 2935 matches em 4.d.3_Python\n",
      "‚úÖ 34 matches em 4.d.13_PHP\n",
      "‚úÖ 223 matches em 4.d.14_JavaScript\n",
      "‚úÖ 2935 matches em 4.d.3_Python\n",
      "‚úÖ 2935 matches em 4.d.3_Python\n",
      "‚úÖ 31 matches em 4.d.5_.NET\n",
      "‚úÖ 130 matches em 4.d.8_SAS/Stata\n",
      "‚úÖ 20 matches em 4.d.11_Matlab\n",
      "‚úÖ 12 matches em 4.d.12_Rust\n",
      "‚úÖ 223 matches em 4.d.14_JavaScript\n",
      "‚úÖ 0 matches em 4.d.15_N√£o utilizo nenhuma das linguagens listadas\n",
      "‚úÖ 2935 matches em 4.d.3_Python\n",
      "‚úÖ 34 matches em 4.d.13_PHP\n",
      "‚úÖ 0 matches em 4.d.15_N√£o utilizo nenhuma das linguagens listadas\n",
      "‚úÖ 2935 matches em 4.d.3_Python\n",
      "‚úÖ 0 matches em 4.d.15_N√£o utilizo nenhuma das linguagens listadas\n",
      "‚úÖ 2935 matches em 4.d.3_Python\n",
      "‚úÖ 31 matches em 4.d.5_.NET\n",
      "‚úÖ 0 matches em 4.d.15_N√£o utilizo nenhuma das linguagens listadas\n",
      "üîç Processando filtro simples: senioridade\n",
      "‚úÖ 1377 matches em 2.g_nivel\n",
      "üîç Processando filtro simples: experiencia_ti\n",
      "‚úÖ 551 matches em 2.j_tempo_de_experiencia_em_ti\n",
      "\n",
      "üéØ Total de candidatos qualificados (todos crit√©rios): 22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.a_token</th>\n",
       "      <th>1.b_genero</th>\n",
       "      <th>1.c_cor/raca/etnia</th>\n",
       "      <th>1.d_pcd</th>\n",
       "      <th>1.g_vive_no_brasil</th>\n",
       "      <th>1.h_pais_onde_mora</th>\n",
       "      <th>1.i_estado_onde_mora</th>\n",
       "      <th>1.l_nivel_de_ensino</th>\n",
       "      <th>1.m_√°rea_de_forma√ß√£o</th>\n",
       "      <th>2.a_situa√ß√£o_de_trabalho</th>\n",
       "      <th>...</th>\n",
       "      <th>8.c.2_Planilhas (Excel, Google Sheets etc).</th>\n",
       "      <th>8.c.3_Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).</th>\n",
       "      <th>8.c.4_Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).</th>\n",
       "      <th>8.c.5_Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).</th>\n",
       "      <th>8.c.6_Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).</th>\n",
       "      <th>8.c.7_Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).</th>\n",
       "      <th>8.c.8_Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).</th>\n",
       "      <th>8.c.9_Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).</th>\n",
       "      <th>8.c.10_Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).</th>\n",
       "      <th>8.c.11_Ferramentas de estat√≠stica avan√ßada como SPSS, SAS, etc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>jf62md7bimws8u2ojf62mdtkuxl7o7wo</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Parda</td>\n",
       "      <td>N√£o</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S√£o Paulo (SP)</td>\n",
       "      <td>Gradua√ß√£o/Bacharelado</td>\n",
       "      <td>Economia/ Administra√ß√£o / Contabilidade / Fina...</td>\n",
       "      <td>Servidor P√∫blico</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>bnyz9rxnd41xs752bnyz9fu3czqgx2ld</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Parda</td>\n",
       "      <td>N√£o</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Para√≠ba (PB)</td>\n",
       "      <td>Mestrado</td>\n",
       "      <td>Estat√≠stica/ Matem√°tica / Matem√°tica Computaci...</td>\n",
       "      <td>Empregado (CLT)</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>q85as1bw2rk483pwq85as2cxny29b5p6</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>Branca</td>\n",
       "      <td>N√£o</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rio Grande do Norte (RN)</td>\n",
       "      <td>Gradua√ß√£o/Bacharelado</td>\n",
       "      <td>Computa√ß√£o / Engenharia de Software / Sistemas...</td>\n",
       "      <td>Empregado (CLT)</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0.a_token 1.b_genero 1.c_cor/raca/etnia 1.d_pcd  \\\n",
       "473  jf62md7bimws8u2ojf62mdtkuxl7o7wo  Masculino              Parda     N√£o   \n",
       "592  bnyz9rxnd41xs752bnyz9fu3czqgx2ld  Masculino              Parda     N√£o   \n",
       "596  q85as1bw2rk483pwq85as2cxny29b5p6  Masculino             Branca     N√£o   \n",
       "\n",
       "     1.g_vive_no_brasil 1.h_pais_onde_mora      1.i_estado_onde_mora  \\\n",
       "473                True                NaN            S√£o Paulo (SP)   \n",
       "592                True                NaN              Para√≠ba (PB)   \n",
       "596                True                NaN  Rio Grande do Norte (RN)   \n",
       "\n",
       "       1.l_nivel_de_ensino                               1.m_√°rea_de_forma√ß√£o  \\\n",
       "473  Gradua√ß√£o/Bacharelado  Economia/ Administra√ß√£o / Contabilidade / Fina...   \n",
       "592               Mestrado  Estat√≠stica/ Matem√°tica / Matem√°tica Computaci...   \n",
       "596  Gradua√ß√£o/Bacharelado  Computa√ß√£o / Engenharia de Software / Sistemas...   \n",
       "\n",
       "    2.a_situa√ß√£o_de_trabalho  ... 8.c.2_Planilhas (Excel, Google Sheets etc).  \\\n",
       "473         Servidor P√∫blico  ...                                         NaN   \n",
       "592          Empregado (CLT)  ...                                         1.0   \n",
       "596          Empregado (CLT)  ...                                         1.0   \n",
       "\n",
       "    8.c.3_Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).  \\\n",
       "473                                                NaN                           \n",
       "592                                                1.0                           \n",
       "596                                                1.0                           \n",
       "\n",
       "    8.c.4_Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).  \\\n",
       "473                                                NaN                                                 \n",
       "592                                                1.0                                                 \n",
       "596                                                1.0                                                 \n",
       "\n",
       "    8.c.5_Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).  \\\n",
       "473                                                NaN              \n",
       "592                                                0.0              \n",
       "596                                                0.0              \n",
       "\n",
       "    8.c.6_Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).  \\\n",
       "473                                                NaN                                \n",
       "592                                                0.0                                \n",
       "596                                                0.0                                \n",
       "\n",
       "    8.c.7_Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).  \\\n",
       "473                                                NaN                                          \n",
       "592                                                1.0                                          \n",
       "596                                                1.0                                          \n",
       "\n",
       "    8.c.8_Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).  \\\n",
       "473                                                NaN                                         \n",
       "592                                                1.0                                         \n",
       "596                                                1.0                                         \n",
       "\n",
       "    8.c.9_Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).  \\\n",
       "473                                                NaN                         \n",
       "592                                                1.0                         \n",
       "596                                                1.0                         \n",
       "\n",
       "    8.c.10_Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).  \\\n",
       "473                                                NaN                     \n",
       "592                                                1.0                     \n",
       "596                                                1.0                     \n",
       "\n",
       "    8.c.11_Ferramentas de estat√≠stica avan√ßada como SPSS, SAS, etc.  \n",
       "473                                                NaN               \n",
       "592                                                0.0               \n",
       "596                                                0.0               \n",
       "\n",
       "[3 rows x 231 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reco = RecomendadorRH(df_selecionado)\n",
    "\n",
    "# Filtros podem ser aplicados se quiser restringir o universo\n",
    "criterios = {\n",
    "    \"cargo_geral\": \"cientista\",\n",
    "    \"modelo_trabalho_ideal\": \"Remoto\",\n",
    "    \"linguagens_programacao\": \"Python\",\n",
    "    \"senioridade\": \"Pleno\",\n",
    "    \"experiencia_ti\": \"2 anos\"\n",
    "    \n",
    "}\n",
    "df_filtrados = reco.aplicar_filtros(criterios)\n",
    "df_filtrados.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62dc9f1",
   "metadata": {},
   "source": [
    "### Define um perfil de vaga ideal\n",
    "- como se fosse o perfil que a empresa est√° buscando, esse dicion√°rio √© usado depois para comparar com os candidatos filtrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5f38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaga_exemplo = {\n",
    "    \"2.f_cargo_atual\": \"Cientista de Dados\",\n",
    "    \"2.g_nivel\": \"Pleno\",\n",
    "    \"2.i_tempo_de_experiencia_em_dados\": \"de 3 a 4 anos\",\n",
    "    \"2.j_tempo_de_experiencia_em_ti\": \"de 5 a 6 anos\",\n",
    "    \"4.d.1_SQL\": 1,\n",
    "    \"4.d.3_Python\": 1,\n",
    "    \"4.h.1_Amazon Web Services (AWS)\": 1,\n",
    "    \"2.s_modelo_de_trabalho_ideal\": \"Remoto\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2d3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de candidatos ap√≥s filtro: 22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total de candidatos ap√≥s filtro: {len(reco.df_filtrado)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f3d7f4",
   "metadata": {},
   "source": [
    "### transforma o dicion√°rio em um vetor bin√°rio ( com o mesmo campos vetorizados que os candidatos)\n",
    "### Compara esse vetor da vaga com os vetores de todos os candidatos filtrados usando cosine_similarity.\n",
    "### Como resultado recebe um dataframe df_ranqueado com os candidatos ordenados por similaridade com a vaga, ou seja, os mais compat√≠veis com o perfil desejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad5d20b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Similaridade calculada e resultados ranqueados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n",
      "c:\\Users\\Tiago\\Desktop\\Projeto-StateofData\\app\\recomendador.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vetor_vaga[col] = 0\n"
     ]
    }
   ],
   "source": [
    "reco.criar_vetor_vaga(vaga_exemplo)\n",
    "df_ranqueado = reco.calcular_similaridade()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4be558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Shape vetor candidatos: (22, 389)\n",
      "üîç Shape vetor vaga: (1, 389)\n",
      "üîç Colunas comuns: {'4.g.11_S3', '8.b_tecnicas_e_metodos_ds_Utilizo modelos de regress√£o (linear, log√≠stica, GLM)., Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o., Utilizo m√©todos estat√≠sticos Bayesianos para analisar dados., Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Realizo previs√µes atrav√©s de modelos de S√©ries Temporais (Time Series).', '8.b.11_Utilizo modelos de Machine Learning para detec√ß√£o de fraude.', '8.a.11_Sou respons√°vel por criar e manter a infra que meus modelos e solu√ß√µes rodam (clusters, servidores, API, containers, etc.)', '0.a_token_n93x0ah3uxc2sqedon93x05s1c0k5avz', '1.i_estado_onde_mora_Cear√° (CE)', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Colaboradores utilizando solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., N√£o tenho visto solu√ß√µes de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso s√£o isolados ou ainda est√£o muito no in√≠cio.', '4.g.16_SQLite', '4.d_linguagem_de_programacao_(dia_a_dia)_R, Python, SQL', '2.r_modelo_de_trabalho_atual_Modelo h√≠brido flex√≠vel (o funcion√°rio tem liberdade para escolher quando estar no escrit√≥rio presencialmente)', '1.i_estado_onde_mora_Esp√≠rito Santo (ES)', '1.m_√°rea_de_forma√ß√£o_Estat√≠stica/ Matem√°tica / Matem√°tica Computacional/ Ci√™ncias Atuariais', '8.b.12_Utilizo m√©todos de Vis√£o Computacional.', '8.c_tecnologias_ds_Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).', '7.b.9_Alteryx', '8.c_tecnologias_ds_Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).', \"8.a_rotina_como_ds_Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio., Sou respons√°vel por colocar modelos em produ√ß√£o, criar os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de neg√≥cio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Sou respons√°vel por entrar em contato com os times de neg√≥cio para defini√ß√£o do problema, identificar a solu√ß√£o e apresenta√ß√£o de resultados., Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem.\", '8.a.5_Sou respons√°vel por colocar modelos em produ√ß√£o, criar os pipelines de dados, APIs de consumo e monitoramento.', '8.a.10_Crio e gerencio solu√ß√µes de Feature Store e cultura de MLOps.', '4.h_cloud_(dia_a_dia)_Google Cloud (GCP), Cloud Pr√≥pria', '1.i_estado_onde_mora_Minas Gerais (MG)', '4.l.5 AI Generativa e LLMs para melhorar produtos internos para os colaboradores', '4.g.26_Snowflake', '4.c.4_Textos/Documentos', '4.h.5_IBM', '4.g_banco_de_dados_(dia_a_dia)_Amazon Athena, S3', '7.c.6_N√£o sei informar.', '4.g.21_Neo4J', '4.c_fonte_de_dado_mais_usada_Dados relacionais (estruturados em bancos SQL), Dados georeferenciados', '2.a_situa√ß√£o_de_trabalho_Servidor P√∫blico', '4.g.1_MySQL', '0.a_token_g6hnse1y56c8ew9tg6hb0f9gz3agvvs6', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Colaboradores utilizando solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando solu√ß√µes no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).', '4.g.22_Google BigQuery', \"8.a.12_Treino e aplico LLM's para solucionar problemas de neg√≥cio.\", '4.d.8_SAS/Stata', '4.a.1_atuacao_em_dados_Outra atua√ß√£o', '7.b.2_SQL & Stored Procedures', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_Utilizo solu√ß√µes pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu pr√≥prio bolso.', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Colaboradores utilizando solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando solu√ß√µes no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de propor melhorias e inova√ß√µes para impulsionar a diferencia√ß√£o de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, servi√ßos etc), Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).', '8.c.2_Planilhas (Excel, Google Sheets etc).', '0.a_token_q85as1bw2rk483pwq85as2cxny29b5p6', '4.h_cloud_(dia_a_dia)_Azure (Microsoft)', '7.b.15_SAP BW ETL', '8.c_tecnologias_ds_Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).', '4.d_linguagem_de_programacao_(dia_a_dia)_Python, SQL, Matlab, PHP', '4.h.7_Cloud Pr√≥pria', '4.g.31_SAP HANA', '4.g.15_Microsoft Access', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Planilhas, Textos/Documentos, Dados georeferenciados', \"8.a_rotina_como_ds_Realizo constru√ß√µes de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Treinando e aplicando LLM's para solucionar problemas de neg√≥cio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem., Sou respons√°vel por entrar em contato com os times de neg√≥cio para defini√ß√£o do problema, identificar a solu√ß√£o e apresenta√ß√£o de resultados., Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Sou respons√°vel por criar e manter a infra que meus modelos e solu√ß√µes rodam (clusters, servidores, API, containers, etc.)\", '4.g.10_Datomic', '4.d_linguagem_de_programacao_(dia_a_dia)_Python', '8.b.2_Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o.', '4.m.4 A empresa que trabalho paga pelas solu√ß√µes de AI Generativa com foco em produtividade', '8.c.10_Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).', '8.b_tecnicas_e_metodos_ds_Utilizo modelos de regress√£o (linear, log√≠stica, GLM)., Utilizo m√©todos estat√≠sticos Bayesianos para analisar dados., Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Realizo previs√µes atrav√©s de modelos de S√©ries Temporais (Time Series).', '4.c_fonte_de_dado_mais_usada_Planilhas', '8.a_rotina_como_ds_Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio.', '2.r_modelo_de_trabalho_atual_Modelo 100% presencial', '4.b_fontes_de_dados_(dia_a_dia)_Planilhas, Dados armazenados em bancos NoSQL, Dados relacionais (estruturados em bancos SQL), Dados georeferenciados, Textos/Documentos', '7.b.7_Talend', \"8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Treinando e aplicando LLM's para solucionar problemas de neg√≥cio., Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio., Crio e dou manuten√ß√£o em ETLs, DAGs e automa√ß√µes de pipelines de dados.\", '1.c_cor/raca/etnia_Parda', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Imagens, Textos/Documentos, Planilhas', '8.c_tecnologias_ds_Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).', '2.f_cargo_atual_Engenheiro de Machine Learning/ML Engineer/AI Engineer', '8.a_rotina_como_ds_Sou respons√°vel por colocar modelos em produ√ß√£o, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio., Sou respons√°vel por criar e manter a infra que meus modelos e solu√ß√µes rodam (clusters, servidores, API, containers, etc.)', '8.b.8_Desenvolvo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc).', '4.d_linguagem_de_programacao_(dia_a_dia)_Python, R', '4.h_cloud_(dia_a_dia)_Cloud Pr√≥pria', '7.c.5_Minha empresa n√£o utiliza essas ferramentas.', '4.c_fonte_de_dado_mais_usada_Textos/Documentos, Planilhas', '8.c_tecnologias_ds_Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).', '4.b.8_Dados georeferenciados', '8.b_tecnicas_e_metodos_ds_Utilizo modelos de regress√£o (linear, log√≠stica, GLM)., Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o., Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo modelos de Machine Learning para detec√ß√£o de fraude.', '4.c_fonte_de_dado_mais_usada_Dados relacionais (estruturados em bancos SQL)', '4.g_banco_de_dados_(dia_a_dia)_DB2, SQLite, S3', '4.g.32_Hive', '8.b.13_Utilizo modelos de Detec√ß√£o de Churn.', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de propor melhorias e inova√ß√µes para impulsionar a diferencia√ß√£o de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, servi√ßos etc)', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Colaboradores utilizando solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando solu√ß√µes no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.', '4.c.2_Dados armazenados em bancos NoSQL', '8.c.3_Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).', '4.d_linguagem_de_programacao_(dia_a_dia)_Python, SQL, Scala', \"8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem., Sou respons√°vel por entrar em contato com os times de neg√≥cio para defini√ß√£o do problema, identificar a solu√ß√£o e apresenta√ß√£o de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de neg√≥cio., Sou respons√°vel por colocar modelos em produ√ß√£o, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manuten√ß√£o em ETLs, DAGs e automa√ß√µes de pipelines de dados.\", '8.c_tecnologias_ds_Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).', '4.a_funcao_de_atuacao_Atuo na √°rea de dados, mas n√£o atuo em nenhuma das frentes citadas.', '0.a_token_2b6zvl5s9vs0ackmu22b6znhc0erf6b5', '0.a_token_w5m7reyike0w71t5528iw5m7reylcdyc', '8.c_tecnologias_ds_Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).', '4.g.29_Presto', '1.i_estado_onde_mora_Rio Grande do Norte (RN)', '1.b_genero_Feminino', 'experiencia_ti_anos', '7.b.17_SAS Data Integration', '8.c.9_Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).', '1.i_estado_onde_mora_Distrito Federal (DF)', '4.d_linguagem_de_programacao_(dia_a_dia)_Python, SQL', '4.h_cloud_(dia_a_dia)_Azure (Microsoft), Amazon Web Services (AWS)', '4.g_banco_de_dados_(dia_a_dia)_Hive, Oracle, Neo4J', '1.l_nivel_de_ensino_Mestrado', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Planilhas, Dados georeferenciados', '4.c.3_Imagens', '8.a.6_Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio.', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Textos/Documentos, Planilhas, Imagens', \"8.a_rotina_como_ds_Sou respons√°vel por entrar em contato com os times de neg√≥cio para defini√ß√£o do problema, identificar a solu√ß√£o e apresenta√ß√£o de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Treinando e aplicando LLM's para solucionar problemas de neg√≥cio., Realizo constru√ß√µes de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\", '4.m.5 Uso solu√ß√µes do tipo Copilot', '8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Utilizo ferramentas avan√ßadas de estat√≠stica como SAS, SPSS, Stata etc, para realizar an√°lises.', '8.c.4_Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).', '4.g.2_Oracle', '7.b.14_IBM DataStage', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_Utilizo solu√ß√µes pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solu√ß√£o.', '4.m.2 Uso solu√ß√µes gratuitas de AI Generativa com foco em produtividade', '1.m_√°rea_de_forma√ß√£o_Outras Engenharias (n√£o incluir engenharia de software ou TI)', '8.a.9_Crio e dou manuten√ß√£o em ETLs, DAGs e automa√ß√µes de pipelines de dados.', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Solu√ß√µes de IA Generatica e LLMs est√£o sendo tratadas como principal frente do neg√≥cio (com o objetivo de substituir o modelo de neg√≥cio atual)', '2.h_faixa_salarial_de R$ 3.001/m√™s a R$ 4.000/m√™s', '8.b.10_Utilizo modelos de Reinforcement Learning (aprendizado por refor√ßo).', '8.b.4_Utilizo m√©todos estat√≠sticos Bayesianos para analisar dados.', '4.d.4_C/C++/C#', '8.b_tecnicas_e_metodos_ds_Utilizo modelos de regress√£o (linear, log√≠stica, GLM)., Realizo previs√µes atrav√©s de modelos de S√©ries Temporais (Time Series).', '4.h_cloud_(dia_a_dia)_Oracle Cloud', '4.g_banco_de_dados_(dia_a_dia)_S3, Oracle, HBase, Databricks, DB2, PostgreSQL, Hive', '2.t_atitude_em_caso_de_retorno_presencial_Vou procurar outra oportunidade no modelo 100% remoto', \"8.a_rotina_como_ds_Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio., Treinando e aplicando LLM's para solucionar problemas de neg√≥cio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem., Sou respons√°vel por colocar modelos em produ√ß√£o, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manuten√ß√£o em ETLs, DAGs e automa√ß√µes de pipelines de dados., Crio e gerencio solu√ß√µes de Feature Store e cultura de MLOps.\", '7.a.8_Desenvolvo/cuido da manuten√ß√£o de planilhas para atender as √°reas de neg√≥cio.', '8.a.7_Realizo constru√ß√µes de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc', '8.c.6_Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).', '4.b.5_V√≠deos', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Dados armazenados em bancos NoSQL, Imagens, Textos/Documentos, √Åudios, Planilhas, Dados georeferenciados, V√≠deos', '4.a_funcao_de_atuacao_*An√°lise de Dados/BI:* Extrai e cruza dados unindo diferentes fontes da informa√ß√£o; analisa dados visando identificar padr√µes, gerar insights e levantar perguntas; desenvolve dashboards, relat√≥rios e visualiza√ß√µes de dados em ferramentas de BI.', '7.a.10_Nenhuma das op√ß√µes listadas refletem meu dia a dia.', '4.l.8 N√£o sei opinar sobre o uso de IA Generativa e LLMs na empresa', '1.h_pais_onde_mora_Portugal', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Imagens, Textos/Documentos, Planilhas, Dados georeferenciados', '8.c_tecnologias_ds_Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estat√≠stica avan√ßada como SPSS, SAS etc.', '4.g.24_Amazon Redshift', '8.a.3_Sou respons√°vel por entrar em contato com os times de neg√≥cio para defini√ß√£o do problema, identificar a solu√ß√£o e apresenta√ß√£o de resultados.', '4.g_banco_de_dados_(dia_a_dia)_SQL SERVER, Google BigQuery', '8.b_tecnicas_e_metodos_ds_Utilizo modelos de regress√£o (linear, log√≠stica, GLM)., Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detec√ß√£o de fraude.', '8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio.', '7.b.19_Knime', '0.a_token_dec7kytj7kzwj01eysws6s3dec7kypsb', '8.b.3_Desenvolvo sistemas de recomenda√ß√£o (RecSys).', '8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio., Realizo constru√ß√µes de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.', '8.c_tecnologias_ds_Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_Utilizo solu√ß√µes no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia.', '4.h.3_Azure (Microsoft)', '4.d.5_.NET', '4.d_linguagem_de_programacao_(dia_a_dia)_SQL, R, Python, JavaScript', '0.a_token_an7f4nm9s23xsmk3sbyfan7ra6jbxyhm', '7.b.13_Oracle Data Integrator', '7.a.3_Crio consultas atrav√©s da linguagem SQL para exportar informa√ß√µes e compartilhar com as √°reas de neg√≥cio.', '0.a_token_55p230wz7ve2n43rkwck55p23bf78zzk', '4.l.3 Desenvolvedores utilizando Copilots', '7.b_ferramentas_etl_da_Oracle Data Integrator, Knime', \"8.b_tecnicas_e_metodos_ds_Utilizo modelos de regress√£o (linear, log√≠stica, GLM)., Utilizo LLM's para solucionar problemas de neg√≥cio., Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o., Utilizo m√©todos estat√≠sticos Bayesianos para analisar dados., Utilizo t√©cnicas de NLP (Natural Language Processing) para an√°lisar dados n√£o-estruturados., Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc).\", '4.b.6_√Åudios', '4.d.9_Visual Basic/VBA', '1.i_estado_onde_mora_Para√≠ba (PB)', '7.b.10_Stitch', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de propor melhorias e inova√ß√µes para impulsionar a diferencia√ß√£o de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, servi√ßos etc)', '0.a_token_6434opep68pewbwbs6434oz5d4zdr9cz', '7.b.21_N√£o utilizo ferramentas de ETL', '8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Sou respons√°vel por entrar em contato com os times de neg√≥cio para defini√ß√£o do problema, identificar a solu√ß√£o e apresenta√ß√£o de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio.', '4.g_banco_de_dados_(dia_a_dia)_PostgreSQL', '2.t_atitude_em_caso_de_retorno_presencial_Vou procurar outra oportunidade no modelo h√≠brido ou remoto', '8.b_tecnicas_e_metodos_ds_Utilizo modelos de regress√£o (linear, log√≠stica, GLM)., Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o., Utilizo m√©todos estat√≠sticos Bayesianos para analisar dados., Utilizo t√©cnicas de NLP (Natural Language Processing) para an√°lisar dados n√£o-estruturados., Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Realizo previs√µes atrav√©s de modelos de S√©ries Temporais (Time Series)., Utilizo modelos de Machine Learning para detec√ß√£o de fraude.', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Textos/Documentos, Planilhas, Dados georeferenciados', '4.b.1_Dados relacionais (estruturados em bancos SQL)', '4.c_fonte_de_dado_mais_usada_Imagens, Dados georeferenciados', '2.h_faixa_salarial_de R$ 8.001/m√™s a R$ 12.000/m√™s', '4.c.5_V√≠deos', '4.h_cloud_(dia_a_dia)_Google Cloud (GCP), Azure (Microsoft)', '4.g_banco_de_dados_(dia_a_dia)_Google BigQuery', '7.b.12_Google Dataflow', '0.a_token_7qco2lywxj33yf415r7qco2lyadtfqlh', '4.c_fonte_de_dado_mais_usada_Planilhas, Dados relacionais (estruturados em bancos SQL)', '4.l.2 Direcionamento centralizado do uso de AI generativa', 'experiencia_dados_anos', '8.c_tecnologias_ds_Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).', '4.b.7_Planilhas', '4.h_cloud_(dia_a_dia)_Google Cloud (GCP), Amazon Web Services (AWS)', '4.a.1_atuacao_em_dados_Ci√™ncia de Dados', '8.a.8_Utilizo ferramentas avan√ßadas de estat√≠stica como SAS, SPSS, Stata etc, para realizar an√°lises.', '4.d.11_Matlab', '8.b_tecnicas_e_metodos_ds_Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o., Desenvolvo sistemas de recomenda√ß√£o (RecSys)., Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc).', '4.g_banco_de_dados_(dia_a_dia)_Amazon Athena, S3, PostgreSQL', '1.d_pcd_N√£o', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_N√£o tenho visto solu√ß√µes de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso s√£o isolados ou ainda est√£o muito no in√≠cio.', '7.c_ferramentas_autonomia_area_de_negocios_N√£o sei informar.', '7.b_ferramentas_etl_da_Scripts Python', '2.a_situa√ß√£o_de_trabalho_Empregado (CLT)', '8.c.7_Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).', '4.g.25_Amazon Athena', '1.l_nivel_de_ensino_Gradua√ß√£o/Bacharelado', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_Utilizo apenas solu√ß√µes gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia., Utilizo solu√ß√µes pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solu√ß√£o.', '8.a_rotina_como_ds_Sou respons√°vel por entrar em contato com os times de neg√≥cio para defini√ß√£o do problema, identificar a solu√ß√£o e apresenta√ß√£o de resultados., Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados).', '8.c_tecnologias_ds_Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).', '0.a_token_3r191w4sidsgkr5hr23r19831bidtrkq', '4.d.12_Rust', '1.g_vive_no_brasil', '4.d.3_Python', '4.g_banco_de_dados_(dia_a_dia)_MongoDB, SQLite, PostgreSQL', '4.g.5_DynamoDB', '8.a.4_Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados).', '0.a_token_bnyz9rxnd41xs752bnyz9fu3czqgx2ld', '4.h.1_Amazon Web Services (AWS)', '1.l_nivel_de_ensino_P√≥s-gradua√ß√£o', '8.c.5_Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_Utilizo solu√ß√µes pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu pr√≥prio bolso., Utilizo solu√ß√µes no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia.', '4.g.12_PostgreSQL', '4.l.6 IA Generativa e LLMs como principal frente do neg√≥cio', '0.a_token_pp5nk7tcp8tt36qresirpp5nk7uz9nm1', '7.b.8_Pentaho', '4.d.13_PHP', '1.m_√°rea_de_forma√ß√£o_Qu√≠mica / F√≠sica', '4.b.4_Textos/Documentos', '0.a_token_s4iymc0mj2sypdtcnqf0s4iym9sq12hi', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Existe um direcionamento centralizado para que colaboradores utilizem solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando solu√ß√µes no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de propor melhorias e inova√ß√µes para impulsionar a diferencia√ß√£o de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, servi√ßos etc), Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).', '4.g.33_Firebird', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).', '4.b_fontes_de_dados_(dia_a_dia)_Dados armazenados em bancos NoSQL, Dados relacionais (estruturados em bancos SQL), Dados georeferenciados', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de propor melhorias e inova√ß√µes para impulsionar a diferencia√ß√£o de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, servi√ßos etc), Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando solu√ß√µes no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.', '2.h_faixa_salarial_de R$ 12.001/m√™s a R$ 16.000/m√™s', \"8.b_tecnicas_e_metodos_ds_Utilizo LLM's para solucionar problemas de neg√≥cio., Utilizo t√©cnicas de NLP (Natural Language Processing) para an√°lisar dados n√£o-estruturados., Utilizo m√©todos de Vis√£o Computacional.\", '1.m_√°rea_de_forma√ß√£o_Economia/ Administra√ß√£o / Contabilidade / Finan√ßas/ Neg√≥cios', '4.g.6_CoachDB', '7.c_ferramentas_autonomia_area_de_negocios_\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc.', '2.r_modelo_de_trabalho_atual_Modelo 100% remoto', '1.b_genero_Masculino', '4.l.4 AI Generativa e LLMs para melhorar produtos externos para os clientes finais', '1.i_estado_onde_mora_Maranh√£o (MA)', '0.a_token_qfg2vpiy1cdwqfghqxfyzfgan6mlg59e', '4.d.14_JavaScript', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de propor melhorias e inova√ß√µes para impulsionar a diferencia√ß√£o de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, servi√ßos etc)', \"8.b_tecnicas_e_metodos_ds_Utilizo LLM's para solucionar problemas de neg√≥cio., Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o., Utilizo t√©cnicas de NLP (Natural Language Processing) para an√°lisar dados n√£o-estruturados., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Utilizo m√©todos de Vis√£o Computacional.\", '4.d.7_Julia', '0.a_token_0muir6cmq7eegfgdv0muir6p99ar65g4', '4.b_fontes_de_dados_(dia_a_dia)_Textos/Documentos, Planilhas', '4.g_banco_de_dados_(dia_a_dia)_SQL SERVER, Hive, Databricks, MySQL', '7.b.1_Scripts Python', '0.a_token_l803cvw0zbqw0pwl25l803cv20hae5vt', '1.c_cor/raca/etnia_Amarela', '4.g.18_Firebase', '4.h_cloud_(dia_a_dia)_IBM, Cloud Pr√≥pria', '1.i_estado_onde_mora_Amazonas (AM)', '1.i_estado_onde_mora_Rio de Janeiro (RJ)', '7.b.6_AWS Glue', '7.a_rotina_como_da_Processo e analiso dados utilizando linguagens de programa√ß√£o como Python, R etc., Realizo constru√ß√µes de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas atrav√©s da linguagem SQL para exportar informa√ß√µes e compartilhar com as √°reas de neg√≥cio.', '7.c.3_Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics.', '7.a.9_Utilizo ferramentas avan√ßadas de estat√≠stica como SAS, SPSS, Stata etc, para realizar an√°lises de dados.', '4.g.19_Vertica', '4.h.6_Servidores On Premise/N√£o utilizamos Cloud', '4.b.2_Dados armazenados em bancos NoSQL', \"7.a_rotina_como_da_Crio consultas atrav√©s da linguagem SQL para exportar informa√ß√µes e compartilhar com as √°reas de neg√≥cio., Desenvolvo/cuido da manuten√ß√£o de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Processo e analiso dados utilizando linguagens de programa√ß√£o como Python, R etc.\", '4.h.4_Oracle Cloud', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_Utilizo solu√ß√µes pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solu√ß√£o., Utilizo solu√ß√µes no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia.', '2.s_modelo_de_trabalho_ideal_Modelo 100% remoto', '2.a_situa√ß√£o_de_trabalho_Vivo fora do Brasil e trabalho para empresa de fora do Brasil', '1.m_√°rea_de_forma√ß√£o_Computa√ß√£o / Engenharia de Software / Sistemas de Informa√ß√£o/ TI', '4.b_fontes_de_dados_(dia_a_dia)_Imagens, V√≠deos, Dados georeferenciados', '8.b_tecnicas_e_metodos_ds_Utilizo modelos de regress√£o (linear, log√≠stica, GLM)., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Utilizo modelos de Detec√ß√£o de Churn.', '4.g.8_MongoDB', '4.h_cloud_(dia_a_dia)_Azure (Microsoft), Servidores On Premise/N√£o utilizamos Cloud', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Dados armazenados em bancos NoSQL, Imagens, Textos/Documentos, Planilhas', '1.c_cor/raca/etnia_Branca', '4.d.10_Scala', '4.c.1_Dados relacionais (estruturados em bancos SQL)', '4.g_banco_de_dados_(dia_a_dia)_Databricks, PostgreSQL', '4.d_linguagem_de_programacao_(dia_a_dia)_SQL, Python', '4.h_cloud_(dia_a_dia)_Servidores On Premise/N√£o utilizamos Cloud', '0.a_token_cj4axe0d5m1zvbp2nlucj4axigcuqufr', '8.c_tecnologias_ds_Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).', '8.a_rotina_como_ds_Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio., Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio.', '7.a.1_Processo e analiso dados utilizando linguagens de programa√ß√£o como Python, R etc.', '8.b_tecnicas_e_metodos_ds_Utilizo t√©cnicas de NLP (Natural Language Processing) para an√°lisar dados n√£o-estruturados., Utilizo modelos de Reinforcement Learning (aprendizado por refor√ßo)., Realizo previs√µes atrav√©s de modelos de S√©ries Temporais (Time Series).', '2.h_faixa_salarial_de R$ 6.001/m√™s a R$ 8.000/m√™s', '7.a.6_Desenvolvo/cuido da manuten√ß√£o de ETL\\\\s utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.', '8.c_tecnologias_ds_Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).', '4.l.7 IA Generativa e LLMs n√£o √© prioridade', '7.b.5_Luigi', '2.r_modelo_de_trabalho_atual_Modelo h√≠brido com dias fixos de trabalho presencial', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_Utilizo apenas solu√ß√µes gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia.', '4.m.1 N√£o uso solu√ß√µes de AI Generativa com foco em produtividade', '0.a_token_jf62md7bimws8u2ojf62mdtkuxl7o7wo', '4.l.1 Colaboradores usando AI generativa de forma independente e descentralizada', '7.b.3_Apache Airflow', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_Utilizo solu√ß√µes no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia., Utilizo apenas solu√ß√µes gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia.', '7.a.7_Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts etc.', '4.c_fonte_de_dado_mais_usada_Dados georeferenciados, Dados relacionais (estruturados em bancos SQL)', '4.h_cloud_(dia_a_dia)_Amazon Web Services (AWS)', '4.c.7_Planilhas', '4.g.3_SQL SERVER', '8.b.6_Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estatistica) para analisar dados.', '0.a_token_xzo94i6zt700vxzo94iebar49e10q6i1', '4.g_banco_de_dados_(dia_a_dia)_S3, PostgreSQL', '1.l_nivel_de_ensino_Doutorado ou Phd', '7.b.16_SQL Server Integration Services (SSIS)', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Uma ou mais equipes testando e aplicando solu√ß√µes de AI Generativa e LLMs, com o objetivo de propor melhorias e inova√ß√µes para impulsionar a diferencia√ß√£o de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, servi√ßos etc), Colaboradores utilizando solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.', '7.b.11_Fivetran', '8.c_tecnologias_ds_Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).', '4.b_fontes_de_dados_(dia_a_dia)_Dados armazenados em bancos NoSQL, Textos/Documentos, Planilhas, Dados relacionais (estruturados em bancos SQL)', \"8.b_tecnicas_e_metodos_ds_Utilizo LLM's para solucionar problemas de neg√≥cio., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo modelos de regress√£o (linear, log√≠stica, GLM).\", '4.m.3 Uso e pago pelas solu√ß√µes de AI Generativa com foco em produtividade', '8.b_tecnicas_e_metodos_ds_Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Realizo previs√µes atrav√©s de modelos de S√©ries Temporais (Time Series).', '7.a.2_Realizo constru√ß√µes de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.', \"8.b_tecnicas_e_metodos_ds_Utilizo LLM's para solucionar problemas de neg√≥cio., Utilizo redes neurais ou modelos baseados em √°rvore para criar modelos de classifica√ß√£o., Utilizo m√©todos de Vis√£o Computacional., Utilizo t√©cnicas de NLP (Natural Language Processing) para an√°lisar dados n√£o-estruturados., Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo modelos de Detec√ß√£o de Churn., Utilizo modelos de Machine Learning para detec√ß√£o de fraude.\", '4.c_fonte_de_dado_mais_usada_Dados relacionais (estruturados em bancos SQL), Textos/Documentos', '8.b.1_Utilizo modelos de regress√£o (linear, log√≠stica, GLM).', '4.b.3_Imagens', '8.c_tecnologias_ds_Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc).', '7.b.20_Databricks', '8.b.5_Utilizo t√©cnicas de NLP (Natural Language Processing) para an√°lisar dados n√£o-estruturados.', '2.f_cargo_atual_Cientista de Dados/Data Scientist', '7.b.18_Qlik Sense', '8.b_tecnicas_e_metodos_ds_Utilizo m√©todos de Vis√£o Computacional.', '4.d_linguagem_de_programacao_(dia_a_dia)_SQL, Python, Visual Basic/VBA', '7.a.5_Realizo experimentos e estudos utilizando metodologias estat√≠sticas como teste de hip√≥tese, modelos de regress√£o etc.', '8.a.1_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio.', '4.d.1_SQL', '4.g_banco_de_dados_(dia_a_dia)_PostgreSQL, Databricks, MySQL, S3', '4.g.23_Google Firestore', '4.g.4_Amazon Aurora ou RDS', '0.a_token_sn912e02enqaj338ix3msn91ji529s7u', '4.d_linguagem_de_programacao_(dia_a_dia)_Python, SQL, SAS/Stata', '2.h_faixa_salarial_de R$ 4.001/m√™s a R$ 6.000/m√™s', '4.g.13_ElasticSearch', '8.b.9_Realizo previs√µes atrav√©s de modelos de S√©ries Temporais (Time Series).', '4.d.6_Java', '4.g.7_Cassandra', '1.i_estado_onde_mora_Rio Grande do Sul (RS)', '4.c_fonte_de_dado_mais_usada_Dados relacionais (estruturados em bancos SQL), Planilhas', '8.b.7_Utilizo cadeias de Markov ou HMM\\\\s para realizar an√°lises de dados.', '1.i_estado_onde_mora_S√£o Paulo (SP)', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Textos/Documentos, Dados georeferenciados, Planilhas, Imagens, Dados armazenados em bancos NoSQL', '8.c_tecnologias_ds_Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de vers√£o (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).', '4.b_fontes_de_dados_(dia_a_dia)_Dados georeferenciados, Dados relacionais (estruturados em bancos SQL), Dados armazenados em bancos NoSQL', '4.g_banco_de_dados_(dia_a_dia)_Redis, Databricks, S3', '7.c.4_Ferramentas de an√°lise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.', '2.a_situa√ß√£o_de_trabalho_Empreendedor ou Empregado (CNPJ)', '4.g.9_MariaDB', '4.h_cloud_(dia_a_dia)_Servidores On Premise/N√£o utilizamos Cloud, Amazon Web Services (AWS)', '8.c.1_Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).', '4.h.2_Google Cloud (GCP)', '4.d.2_R', '4.a.1_atuacao_em_dados_An√°lise de Dados', '0.a_token_z2i0m8cktkhyjcg7v8z2ic1r11idk7p4', '8.c.8_Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_N√£o sei opinar.', '4.d_linguagem_de_programacao_(dia_a_dia)_Python, C/C++/C#', '4.g.14_DB2', '4.g.27_Databricks', '7.c.1_Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.', '4.c.6_√Åudios', '2.g_nivel_Pleno', '4.a_funcao_de_atuacao_*Ci√™ncia de Dados/Machine Learning/AI: *Desenha e executa experimentos com o objetivo de responder perguntas do neg√≥cio; desenvolve modelos preditivos e algoritmos de Machine Learning com o objetivo de otimizar e automatizar a tomada de decis√£o.', '8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio.', '4.c_fonte_de_dado_mais_usada_Dados relacionais (estruturados em bancos SQL), Dados armazenados em bancos NoSQL', '4.g.30_Splunk', '4.g.20_Redis', '7.c.2_\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc.', \"8.b.14_Utilizo LLM's para solucionar problemas de neg√≥cio.\", '8.c.11_Ferramentas de estat√≠stica avan√ßada como SPSS, SAS, etc.', '4.g_banco_de_dados_(dia_a_dia)_MongoDB, PostgreSQL', '1.i_estado_onde_mora_Paran√° (PR)', '8.c_tecnologias_ds_Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).', '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa_Colaboradores utilizando solu√ß√µes baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.', '4.c.8_Dados georeferenciados', '2.t_atitude_em_caso_de_retorno_presencial_Vou aceitar e retornar ao modelo 100% presencial', '7.a.4_Utilizo API\\\\s para extrair dados e complementar minhas an√°lises.', '4.h_cloud_(dia_a_dia)_Amazon Web Services (AWS), Azure (Microsoft), IBM, Cloud Pr√≥pria', '8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados).', '8.b_tecnicas_e_metodos_ds_Utilizo m√©todos estat√≠sticos cl√°ssicos (Testes de hip√≥tese, an√°lise multivariada, sobreviv√™ncia, dados longitudinais, infer√™ncia estat√≠stica) para analisar dados., Utilizo t√©cnicas de Clusteriza√ß√£o (K-means, Spectral, DBScan etc)., Utilizo modelos de Detec√ß√£o de Churn., Utilizo modelos de Machine Learning para detec√ß√£o de fraude.', '8.a_rotina_como_ds_Estudos Ad-hoc com o objetivo de confirmar hip√≥teses, realizar modelos preditivos, forecasts, an√°lise de cluster para resolver problemas pontuais e responder perguntas das √°reas de neg√≥cio., Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem., Sou respons√°vel por entrar em contato com os times de neg√≥cio para defini√ß√£o do problema, identificar a solu√ß√£o e apresenta√ß√£o de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produ√ß√£o em sistemas (produtos de dados)., Cuido da manuten√ß√£o de modelos de Machine Learning j√° em produ√ß√£o, atuando no monitoramento, ajustes e refatora√ß√£o quando necess√°rio.', '4.d.15_N√£o utilizo nenhuma das linguagens listadas', '1.c_cor/raca/etnia_Preta', '8.a.2_Sou respons√°vel pela coleta e limpeza dos dados que uso para an√°lise e modelagem.', '4.h_cloud_(dia_a_dia)_Google Cloud (GCP)', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Textos/Documentos, Planilhas', '1.m_√°rea_de_forma√ß√£o_Ci√™ncias Biol√≥gicas/ Farm√°cia/ Medicina/ √Årea da Sa√∫de', '4.m_usa_chatgpt_ou_copilot_no_trabalho?_N√£o utilizo nenhum tipo de solu√ß√£o de IA Generativa para melhorar a produtividade no dia a dia.', '4.b_fontes_de_dados_(dia_a_dia)_Dados relacionais (estruturados em bancos SQL), Dados armazenados em bancos NoSQL, Textos/Documentos, Dados georeferenciados', '7.b.4_Apache NiFi', '2.d_atua_como_gestor_False', '4.g_banco_de_dados_(dia_a_dia)_SQL SERVER, MySQL', '4.g.17_Sybase', '4.g.28_HBase'}\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Shape vetor candidatos:\", reco.df_vetores.shape)\n",
    "print(\"üîç Shape vetor vaga:\", reco.vetor_vaga.shape)\n",
    "print(\"üîç Colunas comuns:\", set(reco.df_vetores.columns) & set(reco.vetor_vaga.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1793e8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Top 5 candidatos mais compat√≠veis com a vaga:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0e053\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0e053_level0_col0\" class=\"col_heading level0 col0\" >2.f_cargo_atual</th>\n",
       "      <th id=\"T_0e053_level0_col1\" class=\"col_heading level0 col1\" >2.g_nivel</th>\n",
       "      <th id=\"T_0e053_level0_col2\" class=\"col_heading level0 col2\" >2.i_tempo_de_experiencia_em_dados</th>\n",
       "      <th id=\"T_0e053_level0_col3\" class=\"col_heading level0 col3\" >2.j_tempo_de_experiencia_em_ti</th>\n",
       "      <th id=\"T_0e053_level0_col4\" class=\"col_heading level0 col4\" >4.d.1_SQL</th>\n",
       "      <th id=\"T_0e053_level0_col5\" class=\"col_heading level0 col5\" >4.d.3_Python</th>\n",
       "      <th id=\"T_0e053_level0_col6\" class=\"col_heading level0 col6\" >4.h.1_Amazon Web Services (AWS)</th>\n",
       "      <th id=\"T_0e053_level0_col7\" class=\"col_heading level0 col7\" >2.s_modelo_de_trabalho_ideal</th>\n",
       "      <th id=\"T_0e053_level0_col8\" class=\"col_heading level0 col8\" >score_similaridade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0e053_level0_row0\" class=\"row_heading level0 row0\" >4275</th>\n",
       "      <td id=\"T_0e053_row0_col0\" class=\"data row0 col0\" >Cientista de Dados/Data Scientist</td>\n",
       "      <td id=\"T_0e053_row0_col1\" class=\"data row0 col1\" >Pleno</td>\n",
       "      <td id=\"T_0e053_row0_col2\" class=\"data row0 col2\" >Mais de 10 anos</td>\n",
       "      <td id=\"T_0e053_row0_col3\" class=\"data row0 col3\" >de 1 a 2 anos</td>\n",
       "      <td id=\"T_0e053_row0_col4\" class=\"data row0 col4\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row0_col5\" class=\"data row0 col5\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "      <td id=\"T_0e053_row0_col7\" class=\"data row0 col7\" >Modelo 100% remoto</td>\n",
       "      <td id=\"T_0e053_row0_col8\" class=\"data row0 col8\" >56.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e053_level0_row1\" class=\"row_heading level0 row1\" >4966</th>\n",
       "      <td id=\"T_0e053_row1_col0\" class=\"data row1 col0\" >Cientista de Dados/Data Scientist</td>\n",
       "      <td id=\"T_0e053_row1_col1\" class=\"data row1 col1\" >Pleno</td>\n",
       "      <td id=\"T_0e053_row1_col2\" class=\"data row1 col2\" >Mais de 10 anos</td>\n",
       "      <td id=\"T_0e053_row1_col3\" class=\"data row1 col3\" >de 1 a 2 anos</td>\n",
       "      <td id=\"T_0e053_row1_col4\" class=\"data row1 col4\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row1_col5\" class=\"data row1 col5\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
       "      <td id=\"T_0e053_row1_col7\" class=\"data row1 col7\" >Modelo 100% remoto</td>\n",
       "      <td id=\"T_0e053_row1_col8\" class=\"data row1 col8\" >56.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e053_level0_row2\" class=\"row_heading level0 row2\" >3744</th>\n",
       "      <td id=\"T_0e053_row2_col0\" class=\"data row2 col0\" >Cientista de Dados/Data Scientist</td>\n",
       "      <td id=\"T_0e053_row2_col1\" class=\"data row2 col1\" >Pleno</td>\n",
       "      <td id=\"T_0e053_row2_col2\" class=\"data row2 col2\" >de 5 a 6 anos</td>\n",
       "      <td id=\"T_0e053_row2_col3\" class=\"data row2 col3\" >de 1 a 2 anos</td>\n",
       "      <td id=\"T_0e053_row2_col4\" class=\"data row2 col4\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row2_col5\" class=\"data row2 col5\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row2_col6\" class=\"data row2 col6\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row2_col7\" class=\"data row2 col7\" >Modelo 100% remoto</td>\n",
       "      <td id=\"T_0e053_row2_col8\" class=\"data row2 col8\" >51.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e053_level0_row3\" class=\"row_heading level0 row3\" >3059</th>\n",
       "      <td id=\"T_0e053_row3_col0\" class=\"data row3 col0\" >Cientista de Dados/Data Scientist</td>\n",
       "      <td id=\"T_0e053_row3_col1\" class=\"data row3 col1\" >Pleno</td>\n",
       "      <td id=\"T_0e053_row3_col2\" class=\"data row3 col2\" >de 5 a 6 anos</td>\n",
       "      <td id=\"T_0e053_row3_col3\" class=\"data row3 col3\" >de 1 a 2 anos</td>\n",
       "      <td id=\"T_0e053_row3_col4\" class=\"data row3 col4\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row3_col5\" class=\"data row3 col5\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n",
       "      <td id=\"T_0e053_row3_col7\" class=\"data row3 col7\" >Modelo 100% remoto</td>\n",
       "      <td id=\"T_0e053_row3_col8\" class=\"data row3 col8\" >48.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e053_level0_row4\" class=\"row_heading level0 row4\" >1267</th>\n",
       "      <td id=\"T_0e053_row4_col0\" class=\"data row4 col0\" >Cientista de Dados/Data Scientist</td>\n",
       "      <td id=\"T_0e053_row4_col1\" class=\"data row4 col1\" >Pleno</td>\n",
       "      <td id=\"T_0e053_row4_col2\" class=\"data row4 col2\" >de 5 a 6 anos</td>\n",
       "      <td id=\"T_0e053_row4_col3\" class=\"data row4 col3\" >de 1 a 2 anos</td>\n",
       "      <td id=\"T_0e053_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row4_col5\" class=\"data row4 col5\" >1.000000</td>\n",
       "      <td id=\"T_0e053_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
       "      <td id=\"T_0e053_row4_col7\" class=\"data row4 col7\" >Modelo 100% remoto</td>\n",
       "      <td id=\"T_0e053_row4_col8\" class=\"data row4 col8\" >48.09%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18973778440>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üîù Exibir os 5 melhores candidatos com colunas importantes\n",
    "colunas_visiveis = [\n",
    "    \"2.f_cargo_atual\", \"2.g_nivel\", \n",
    "    \"2.i_tempo_de_experiencia_em_dados\", \"2.j_tempo_de_experiencia_em_ti\", \n",
    "    \"4.d.1_SQL\", \"4.d.3_Python\", \"4.h.1_Amazon Web Services (AWS)\", \n",
    "    \"2.s_modelo_de_trabalho_ideal\", \"score_similaridade\"\n",
    "]\n",
    "\n",
    "# üìã Selecionar as colunas que existem no DataFrame (algumas podem faltar)\n",
    "colunas_presentes = [col for col in colunas_visiveis if col in df_ranqueado.columns]\n",
    "\n",
    "# üìä Exibir os top 5 com essas colunas\n",
    "top5 = df_ranqueado[colunas_presentes].sort_values(by=\"score_similaridade\", ascending=False).head(5)\n",
    "\n",
    "# ‚úÖ Exibir\n",
    "print(\"üß† Top 5 candidatos mais compat√≠veis com a vaga:\\n\")\n",
    "display(top5.style.format({\n",
    "    \"score_similaridade\": \"{:.2%}\"\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe58b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Explica√ß√£o gerada:\n",
      "\n",
      "Vaga: - atual: Cientista de Dados - nivel: Pleno - dados: de 3 a 4 anos - ti: de 5 a 6 anos - SQL: 1 - Python: 1 - AWS: 1 - ideal: Remoto  Candidato: - token: sn912e02enqaj338ix3ms\n"
     ]
    }
   ],
   "source": [
    "from app.modelo_llm import gerar_explicacao_llm\n",
    "\n",
    "# Define os dados da vaga usados no match\n",
    "vaga_exemplo = {\n",
    "    \"2.f_cargo_atual\": \"Cientista de Dados\",\n",
    "    \"2.g_nivel\": \"Pleno\",\n",
    "    \"2.i_tempo_de_experiencia_em_dados\": \"de 3 a 4 anos\",\n",
    "    \"2.j_tempo_de_experiencia_em_ti\": \"de 5 a 6 anos\",\n",
    "    \"4.d.1_SQL\": 1,\n",
    "    \"4.d.3_Python\": 1,\n",
    "    \"4.h.1_AWS\": 1,\n",
    "    \"2.s_modelo_de_trabalho_ideal\": \"Remoto\"\n",
    "}\n",
    "\n",
    "# Pega o primeiro candidato ranqueado\n",
    "candidato_top1 = df_ranqueado.iloc[0].to_dict()\n",
    "\n",
    "# Gera explica√ß√£o\n",
    "explicacao = gerar_explicacao_llm(vaga_exemplo, candidato_top1)\n",
    "print(\"üß† Explica√ß√£o gerada:\\n\")\n",
    "print(explicacao)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8c23c",
   "metadata": {},
   "source": [
    "# WIDGETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è Preencha os filtros √† esquerda e defina os requisitos da vaga √† direita:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cf4cafc4e14f84a2a5fd26ac64e362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HTML(value='<b>üéØ Filtros</b>'), Dropdown(description='Cargo:', options=('cientis‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94358b1bf2e40c489203f9bea3efd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='üîç Buscar candidatos', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beefa6b6e5844ed083e8132227031003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from app.recomendador import RecomendadorRH\n",
    "from utils.seletores import selecionar_colunas_recomendador\n",
    "from utils.vetorizacao import vetorizar_perfis\n",
    "from app.modelo_llm import gerar_explicacao_llm\n",
    "from utils.filtros import FILTROS_DISPONIVEIS\n",
    "\n",
    "# ======================================\n",
    "# üìä Carregar dados e preparar sistema\n",
    "# ======================================\n",
    "df_selecionado = selecionar_colunas_recomendador(df)\n",
    "recomendador = RecomendadorRH(df_selecionado)\n",
    "\n",
    "# ======================================\n",
    "# üéØ Widgets de FILTRO\n",
    "# ======================================\n",
    "cargo_dropdown = widgets.Dropdown(\n",
    "    options=[\"cientista\", \"engenheiro\", \"analista\"],\n",
    "    description=\"Cargo:\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "linguagens_checkbox = widgets.SelectMultiple(\n",
    "    options=[col.split(\"_\")[1] for col in FILTROS_DISPONIVEIS[\"linguagens_programacao\"]],\n",
    "    description=\"Linguagens:\"\n",
    ")\n",
    "\n",
    "senioridade_dropdown = widgets.Dropdown(\n",
    "    options=sorted(df_selecionado[\"2.g_nivel\"].dropna().unique()),\n",
    "    description=\"Senioridade:\"\n",
    ")\n",
    "\n",
    "exp_ti_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        \"Menos de 1 ano\", \"de 1 a 2 anos\", \"de 2 a 3 anos\", \"de 3 a 4 anos\",\n",
    "        \"de 4 a 5 anos\", \"de 5 a 6 anos\", \"Mais de 10 anos\"\n",
    "    ],\n",
    "    description=\"Exp. TI:\"\n",
    ")\n",
    "\n",
    "# ======================================\n",
    "# üìå Widgets da VAGA IDEAL (novos)\n",
    "# ======================================\n",
    "vaga_cargo = widgets.Text(value=\"Cientista de Dados\", description=\"Cargo ideal:\")\n",
    "vaga_senioridade = widgets.Dropdown(options=senioridade_dropdown.options, description=\"N√≠vel:\")\n",
    "vaga_exp_dados = widgets.Dropdown(options=exp_ti_dropdown.options, description=\"Exp. Dados:\")\n",
    "vaga_exp_ti = widgets.Dropdown(options=exp_ti_dropdown.options, description=\"Exp. TI:\")\n",
    "\n",
    "vaga_linguagens = widgets.SelectMultiple(\n",
    "    options=FILTROS_DISPONIVEIS[\"linguagens_programacao\"],\n",
    "    description=\"Langs vaga:\"\n",
    ")\n",
    "\n",
    "vaga_clouds = widgets.SelectMultiple(\n",
    "    options=FILTROS_DISPONIVEIS[\"clouds\"],\n",
    "    description=\"Clouds vaga:\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# üîò Bot√£o e l√≥gica de execu√ß√£o\n",
    "# ======================================\n",
    "botao_executar = widgets.Button(description=\"üîç Buscar candidatos\", button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "def ao_clicar(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "\n",
    "        # üéØ Filtros para restringir universo\n",
    "        criterios = {\n",
    "            \"cargo_geral\": cargo_dropdown.value,\n",
    "            \n",
    "            \"senioridade\": senioridade_dropdown.value,\n",
    "            \"experiencia_ti\": exp_ti_dropdown.value,\n",
    "            \"linguagens_programacao\": [\n",
    "                f\"4.d.{idx}_{lang}\" for idx, lang in enumerate(linguagens_checkbox.value, 1)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        df_filtrados = recomendador.aplicar_filtros(criterios)\n",
    "\n",
    "        print(\"\\nüöÄ Rodando sistema completo...\\n\")\n",
    "\n",
    "        # üìå Construindo vaga ideal com base nos widgets\n",
    "        vaga_exemplo = {\n",
    "            \"2.f_cargo_atual\": vaga_cargo.value,\n",
    "            \"2.g_nivel\": vaga_senioridade.value,\n",
    "            \"2.i_tempo_de_experiencia_em_dados\": vaga_exp_dados.value,\n",
    "            \"2.j_tempo_de_experiencia_em_ti\": vaga_exp_ti.value,\n",
    "            \n",
    "        }\n",
    "\n",
    "        # Linguagens e clouds marcadas como 1\n",
    "        for col in vaga_linguagens.value:\n",
    "            vaga_exemplo[col] = 1\n",
    "        for col in vaga_clouds.value:\n",
    "            vaga_exemplo[col] = 1\n",
    "\n",
    "        # üß† Recomenda√ß√£o\n",
    "        recomendador.criar_vetor_vaga(vaga_exemplo)\n",
    "        df_ranqueado = recomendador.calcular_similaridade()\n",
    "\n",
    "        # üéØ Top 5\n",
    "        colunas_visiveis = [\n",
    "            \"2.f_cargo_atual\", \"2.g_nivel\", \n",
    "            \"2.i_tempo_de_experiencia_em_dados\", \"2.j_tempo_de_experiencia_em_ti\",\n",
    "            *vaga_linguagens.value, *vaga_clouds.value,\n",
    "            \"2.s_modelo_de_trabalho_ideal\", \"score_similaridade\"\n",
    "        ]\n",
    "        colunas_presentes = [col for col in colunas_visiveis if col in df_ranqueado.columns]\n",
    "        top5 = df_ranqueado[colunas_presentes].sort_values(by=\"score_similaridade\", ascending=False).head(5)\n",
    "\n",
    "        print(\"üß† Top 5 candidatos mais compat√≠veis com a vaga:\\n\")\n",
    "        display(top5.style.format({\"score_similaridade\": \"{:.2%}\"}))\n",
    "\n",
    "        # Explica√ß√£o LLM\n",
    "        candidato_top1 = df_ranqueado.iloc[0].to_dict()\n",
    "        explicacao = gerar_explicacao_llm(vaga_exemplo, candidato_top1)\n",
    "        print(\"\\nüß† Explica√ß√£o para o 1¬∫ candidato:\")\n",
    "        print(explicacao)\n",
    "\n",
    "botao_executar.on_click(ao_clicar)\n",
    "\n",
    "# ======================================\n",
    "# üß© Layout final do sistema\n",
    "# ======================================\n",
    "print(\"üéõÔ∏è Preencha os filtros √† esquerda e defina os requisitos da vaga √† direita:\")\n",
    "\n",
    "box_filtros = widgets.VBox([\n",
    "    widgets.HTML(value=\"<b>üéØ Filtros</b>\"),\n",
    "    cargo_dropdown, \n",
    "    linguagens_checkbox, senioridade_dropdown, exp_ti_dropdown\n",
    "])\n",
    "\n",
    "box_vaga = widgets.VBox([\n",
    "    widgets.HTML(value=\"<b>üìå Vaga ideal</b>\"),\n",
    "    vaga_cargo, vaga_senioridade, vaga_exp_dados, vaga_exp_ti,\n",
    "    vaga_linguagens, vaga_clouds, \n",
    "])\n",
    "\n",
    "display(widgets.HBox([box_filtros, box_vaga]))\n",
    "display(botao_executar)\n",
    "display(output)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
